{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploring Reinforcement Learning by Building a ConnextX Agent\n",
    "Connect X is a competition hosted by Kaggle, requiring us to build an agent\n",
    "which is able to win a \"Connect 4\" game\n",
    "\n",
    "https://www.kaggle.com/competitions/connectx/overview\n",
    "\n",
    "In this notebook we want to develop different reinforcement learning\n",
    "algorithms from scratch in order to build a reasonably strong agent.\n",
    "\n",
    "The goal is not to place high in the competition, but to gain a deeper insight into the\n",
    "area of reinforcement learning\n",
    "\n",
    "# Problem Definition\n",
    "## Agent & Action Space\n",
    "The agent needs to perform an action. The action space consists of 7 possible actions, since the agent can select either of the 7 columns to place the bead.\n",
    "\n",
    "## Reward\n",
    "Currently, the game context provides a reward of 1 for winning a game and a\n",
    "reward of 0 for losing a game. Therefore, an action according to the game\n",
    "context is equal to playing one game.\n",
    "\n",
    "# How we move forward\n",
    "We will start with the simplest possible agent. Then we will develop a agent which\n",
    "is able to beat the previous agent. That way, we can try out different ways of\n",
    "implementing reinforcement learning and evaluate them by letting two agents play\n",
    "against each other.\n",
    "\n",
    "Hopefully, we can come up with an agent which performs reasonably well against\n",
    "the other agents that are submitted to the competition.\n",
    "\n",
    "# The first Agent\n",
    "The first agent, which is also the simplest one, is the random agent. It chooses\n",
    "a random, non-full column to place its bead. It is also the agent used as an\n",
    "example in the \"Getting Started\" Notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "\n",
    "def random_agent(observation, configuration):\n",
    "    c = configuration\n",
    "    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For testing, let's have two random agents play against each other"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkaggle_environments\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m evaluate, make, utils\n\u001B[0;32m      3\u001B[0m env \u001B[38;5;241m=\u001B[39m make(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconnectx\u001B[39m\u001B[38;5;124m'\u001B[39m, debug\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 5\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrandom_agent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_agent\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m env\u001B[38;5;241m.\u001B[39mrender(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mipython\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Repositories\\ConnectFourAgent\\venv\\Lib\\site-packages\\kaggle_environments\\core.py:267\u001B[0m, in \u001B[0;36mEnvironment.run\u001B[1;34m(self, agents)\u001B[0m\n\u001B[0;32m    265\u001B[0m start \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;129;01mand\u001B[39;00m perf_counter() \u001B[38;5;241m-\u001B[39m start \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfiguration\u001B[38;5;241m.\u001B[39mrunTimeout:\n\u001B[1;32m--> 267\u001B[0m     actions, logs \u001B[38;5;241m=\u001B[39m \u001B[43mrunner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(actions, logs)\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;129;01mand\u001B[39;00m perf_counter() \u001B[38;5;241m-\u001B[39m start \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfiguration\u001B[38;5;241m.\u001B[39mrunTimeout:\n",
      "File \u001B[1;32m~\\Repositories\\ConnectFourAgent\\venv\\Lib\\site-packages\\kaggle_environments\\core.py:697\u001B[0m, in \u001B[0;36mEnvironment.__agent_runner.<locals>.act\u001B[1;34m(none_action)\u001B[0m\n\u001B[0;32m    695\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool\u001B[38;5;241m.\u001B[39mmap(act_agent, act_args)\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 697\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mact_agent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mact_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    699\u001B[0m \u001B[38;5;66;03m# results is a list of tuples where the first element is an agent action and the second is the agent log\u001B[39;00m\n\u001B[0;32m    700\u001B[0m \u001B[38;5;66;03m# This destructures into two lists, a list of actions and a list of logs.\u001B[39;00m\n\u001B[0;32m    701\u001B[0m actions, logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mresults)\n",
      "File \u001B[1;32m~\\Repositories\\ConnectFourAgent\\venv\\Lib\\site-packages\\kaggle_environments\\core.py:118\u001B[0m, in \u001B[0;36mact_agent\u001B[1;34m(args)\u001B[0m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m none_action, {}\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mobservation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Repositories\\ConnectFourAgent\\venv\\Lib\\site-packages\\kaggle_environments\\agent.py:159\u001B[0m, in \u001B[0;36mAgent.act\u001B[1;34m(self, observation)\u001B[0m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    158\u001B[0m     start \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[1;32m--> 159\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc(file\u001B[38;5;241m=\u001B[39merr_buffer)\n",
      "Cell \u001B[1;32mIn[12], line 5\u001B[0m, in \u001B[0;36mrandom_agent\u001B[1;34m(observation, configuration)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrandom_agent\u001B[39m(observation, configuration):\n\u001B[1;32m----> 5\u001B[0m     c \u001B[38;5;241m=\u001B[39m \u001B[43mconfiguration\u001B[49m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m choice([c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(configuration\u001B[38;5;241m.\u001B[39mcolumns) \u001B[38;5;28;01mif\u001B[39;00m observation\u001B[38;5;241m.\u001B[39mboard[c] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[1;32mIn[12], line 5\u001B[0m, in \u001B[0;36mrandom_agent\u001B[1;34m(observation, configuration)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrandom_agent\u001B[39m(observation, configuration):\n\u001B[1;32m----> 5\u001B[0m     c \u001B[38;5;241m=\u001B[39m \u001B[43mconfiguration\u001B[49m\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m choice([c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(configuration\u001B[38;5;241m.\u001B[39mcolumns) \u001B[38;5;28;01mif\u001B[39;00m observation\u001B[38;5;241m.\u001B[39mboard[c] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\231.8770.66\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_frame.py:747\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[1;34m(self, frame, event, arg)\u001B[0m\n\u001B[0;32m    745\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[1;32m--> 747\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    748\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[0;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\231.8770.66\\plugins\\python\\helpers\\pydev\\_pydevd_bundle\\pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\231.8770.66\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\231.8770.66\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "\n",
    "env = make('connectx', debug=True)\n",
    "\n",
    "env.run([random_agent, random_agent])\n",
    "env.render(mode='ipython')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The first Idea: The omniscient Agent\n",
    "As of the time of writing, I don't know a lot about RL. I know about the basics\n",
    "and about how an aged based on the Markov Decision Process works. Therefore,\n",
    "the first idea would be to change the scope of the learning process. Instead of\n",
    "rewarding wins and losses, we would reward individual actions.\n",
    "Any actions that would bring the agent closer to any win condition (or the\n",
    " opponent further away from their win condition) would\n",
    "yield a positive reward, any other action would yield a negative reward. However,\n",
    "the markov chain to accomplish this would need to know any possible state of the\n",
    "game.\n",
    "\n",
    "## How many states are possible\n",
    "For my first attempt, I want to know if this idea is even remotely realistic.\n",
    "Calculating the amount of all possible states is difficult by hand. Therefore\n",
    "I will simply provide the solution and quote its source:\n",
    "\n",
    "$$\n",
    "7.1\\cdot10^{13}=71,000,000,000,000\n",
    "$$\n",
    "(A Knowledge-based Approach of Connect-Four, Victor Allis, 1988)\n",
    "\n",
    "So there are about 71 trillion possible states. If we encode a state, we need\n",
    "to encode at least 42 2-bit values for each of the 3 possible states of one\n",
    " position [blue, white, empty] and at least 7 1-bit values for the reward-edge\n",
    " to the 7 possible subsequent states. To encode an entire markov chain with all possible states, we would need encode\n",
    "$$\n",
    "71,000,000,000,000\\cdot\\left(42\\cdot2+7\\right)=6,461,000,000,000,000\\:bits=807.6\\:TB\n",
    "$$\n",
    "\n",
    "## Conclusion\n",
    "If we had access to a supercomputer, building a omniscient agent might be possible,\n",
    "but we clearly need to be able to run our agent inside a kaggle notebook. Therefore,\n",
    "we need to find a different strategy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Second Idea: Pointing the Agent in the right Direction\n",
    "The next idea doesn't care about the entire state of the game. We can generalize\n",
    "the goal of the game to \"Try to connect 4 and prevent the opponent to connect 4\".\n",
    "\n",
    "So, lets try to reward actions that get the agents closer to the goal using\n",
    "the following value table:\n",
    "\n",
    "| State                         | Value |\n",
    "| ----------------------------- | ----- |\n",
    "| Connect 1                     | 0/6   |\n",
    "| Connect 2                     | 2/6   |\n",
    "| Connect 3                     | 4/6   |\n",
    "| Connect 4                     | 6/6   |\n",
    "| Prevent Opponent to Connect 1 | 0/6   |\n",
    "| Prevent Opponent to Connect 2 | 1/6   |\n",
    "| Prevent Opponent to Connect 3 | 3/6   |\n",
    "| Prevent Opponent to Connect 4 | 5/6   |\n",
    "\n",
    "In that scenario, we reward the agent for winning the game and for preventing\n",
    "the opponent to win the game. The closer to a win a state is, the higher the\n",
    "reward, though winning yourself will be rewarded higher than preventing the\n",
    "opponent to win (if both players have connected 3 disks so far, winning the\n",
    "game in the current turn is more important than preventing the opponent to win\n",
    "the game in their next turn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def simple_agent(observation, configuration):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
